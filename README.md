# MOSA: Music mOtion and Semantic Annotation dataset

![alt text](https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset/blob/main/figure/dataset.png)



<p align="center">
https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset/assets/42167635/13426b3c-1133-4a4d-96cd-860e6b772b1f
</p>




MOSA dataset is a large-scale music dataset containing 742 professional piano and violin solo music performances with 23 musicians (> 30 hours, and > 570 K notes). This dataset features following types of data
- High-quality 3-D motion capture data
- Audio recordings
- Manual semantic annotations


# 1. 3-D motion capture data
The 3-D motion capture data in this dataset are recorded using Qualisys and Vicon 3-D motion capture system. The original coordinates of 34 body/instrument markers on the x-, y-, and z- axes are provided. The body marker placement
follows the Plug-In Gait Full-body model in Visual-3D's official documention.

![alt text](https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset/blob/main/figure/mocap.png)

